{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPliD9TCQe0NnsEzZFTvL2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michellechen202212/journalpaper/blob/main/journalpaper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Increased dataset size → Now 50,000+ authentication events.\n",
        "Explicitly includes → Brute Force, Token Hijacking, MFA Bypass, Impossible Travel, and Pass-the-Hash (PtH) attacks to match the abstract.\n",
        "Balanced anomaly ratio → 75% normal logins, 25% attack scenarios."
      ],
      "metadata": {
        "id": "dBZZFG12sQKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "1vDK2rJsXF0w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MYTQde-krDdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Define synthetic data parameters\n",
        "num_samples = 50000  # Increase dataset size to 50,000+ authentication events\n",
        "users = [f\"user_{i}@company.com\" for i in range(1000)]  # 1000 unique users\n",
        "devices = [\"Workstation\", \"Mobile\", \"VPN\"]\n",
        "locations = [\"New York\", \"San Francisco\", \"London\", \"Tokyo\", \"Berlin\"]\n",
        "auth_methods = [\"OAuth2\", \"SAML\", \"MFA\", \"NTLM\"]\n",
        "anomaly_types = [\"Brute Force\", \"Token Hijacking\", \"MFA Bypass\", \"Impossible Travel\", \"Pass-the-Hash\", \"None\"]\n",
        "\n",
        "# Generate timestamps over 90 days\n",
        "start_date = datetime.now() - timedelta(days=90)\n",
        "timestamps = [start_date + timedelta(minutes=random.randint(0, 129600)) for _ in range(num_samples)]\n",
        "\n",
        "# Create the synthetic dataset\n",
        "df = pd.DataFrame({\n",
        "    \"timestamp\": timestamps,\n",
        "    \"user_principal_name\": np.random.choice(users, size=num_samples),\n",
        "    \"device_type\": np.random.choice(devices, size=num_samples),\n",
        "    \"location\": np.random.choice(locations, size=num_samples),\n",
        "    \"authentication_type\": np.random.choice(auth_methods, size=num_samples),\n",
        "    \"anomaly_type\": np.random.choice(anomaly_types, size=num_samples, p=[0.05, 0.05, 0.05, 0.05, 0.05, 0.75])  # 75% normal logins\n",
        "})\n",
        "\n",
        "# Save the dataset\n",
        "df.to_csv(\"sample_data/synthetic_azure_ad_logs.csv\", index=False)\n",
        "print(\"Updated Synthetic Azure AD authentication dataset generated successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4UaKvocNsOEJ",
        "outputId": "c3ce04a1-c638-4799-d9ea-62314db2861d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated Synthetic Azure AD authentication dataset generated successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NVvyOWqEW6JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the synthetic dataset\n",
        "df = pd.read_csv(\"sample_data/synthetic_azure_ad_logs.csv\")\n",
        "\n",
        "# Extract time-based features\n",
        "df[\"hour\"] = pd.to_datetime(df[\"timestamp\"]).dt.hour\n",
        "df[\"day_of_week\"] = pd.to_datetime(df[\"timestamp\"]).dt.dayofweek\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoders = {}\n",
        "for col in [\"user_principal_name\", \"device_type\", \"location\", \"authentication_type\"]:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "df[[\"user_principal_name\", \"device_type\", \"location\", \"authentication_type\", \"hour\", \"day_of_week\"]] = scaler.fit_transform(\n",
        "    df[[\"user_principal_name\", \"device_type\", \"location\", \"authentication_type\", \"hour\", \"day_of_week\"]])\n",
        "\n",
        "# Clean dataset: Adjust DBSCAN to avoid removing true anomalies\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=10).fit(df.drop(columns=[\"label\"], errors=\"ignore\"))\n",
        "df[\"cluster\"] = dbscan.labels_\n",
        "df = df[df[\"cluster\"] != -1]  # Keep only well-defined clusters (remove excessive filtering)\n",
        "\n",
        "# Define labels for anomaly detection (1 = anomaly, 0 = normal)\n",
        "df[\"label\"] = df[\"anomaly_type\"].apply(lambda x: 1 if x != \"None\" else 0)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(columns=[\"anomaly_type\", \"timestamp\", \"label\", \"cluster\"]), df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM Autoencoder Model\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.3)  # Ensure dropout is applied\n",
        "        self.decoder = nn.LSTM(hidden_dim, input_dim, num_layers=2, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.encoder(x)\n",
        "        decoded, _ = self.decoder(hidden.repeat(x.shape[1], 1, 1))\n",
        "        return decoded.squeeze()  # Ensure correct output shape\n",
        "\n",
        "# Train LSTM Autoencoder\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_dim = X_train.shape[1]\n",
        "model = LSTMAutoencoder(input_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Training loop with final shape fix\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_tensor.unsqueeze(1)).squeeze()  # Ensure input shape is correct\n",
        "    loss = criterion(output, X_train_tensor)  # Ensure target and input shapes match\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item()}\")\n",
        "\n",
        "# Save the trained LSTM Autoencoder\n",
        "torch.save(model.state_dict(), \"lstm_autoencoder.pth\")\n",
        "\n",
        "# Train Isolation Forest with increased contamination to improve recall\n",
        "iso_forest = IsolationForest(contamination=0.45, random_state=42)\n",
        "iso_forest.fit(X_train)\n",
        "iso_forest_preds = iso_forest.predict(X_test)\n",
        "iso_forest_preds = np.where(iso_forest_preds == -1, 1, 0)\n",
        "\n",
        "# Train One-Class SVM with improved `nu` parameter\n",
        "svm_model = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.50)\n",
        "svm_model.fit(X_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_preds = np.where(svm_preds == -1, 1, 0)\n",
        "\n",
        "# Train Local Outlier Factor (LOF) for additional anomaly detection\n",
        "lof_model = LocalOutlierFactor(n_neighbors=10, contamination=0.40)\n",
        "lof_preds = lof_model.fit_predict(X_test)\n",
        "lof_preds = np.where(lof_preds == -1, 1, 0)\n",
        "\n",
        "# Save the trained Isolation Forest, One-Class SVM, and LOF models\n",
        "joblib.dump(iso_forest, \"isolation_forest.pkl\")\n",
        "joblib.dump(svm_model, \"one_class_svm.pkl\")\n",
        "joblib.dump(lof_model, \"local_outlier_factor.pkl\")\n",
        "\n",
        "# Compute evaluation metrics for LSTM Autoencoder\n",
        "with torch.no_grad():\n",
        "    reconstructed = model(X_test_tensor.unsqueeze(1)).squeeze()\n",
        "    reconstruction_errors = torch.mean((X_test_tensor - reconstructed) ** 2, dim=1).cpu().numpy()\n",
        "\n",
        "# Ensure reconstruction_errors is 1D (length = num_samples)\n",
        "reconstruction_errors = reconstruction_errors.flatten()\n",
        "\n",
        "# Define a dynamic threshold for anomaly detection using LSTM Autoencoder (adaptive percentile)\n",
        "lstm_dynamic_thresholds = np.percentile(reconstruction_errors, np.linspace(50, 95, num=5))\n",
        "best_threshold = min(lstm_dynamic_thresholds, key=lambda x: abs(x - np.median(reconstruction_errors)))\n",
        "lstm_preds = (reconstruction_errors > best_threshold).astype(int)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "lstm_precision = precision_score(y_test, lstm_preds)\n",
        "lstm_recall = recall_score(y_test, lstm_preds)\n",
        "lstm_f1 = f1_score(y_test, lstm_preds)\n",
        "\n",
        "iso_precision = precision_score(y_test, iso_forest_preds)\n",
        "iso_recall = recall_score(y_test, iso_forest_preds)\n",
        "iso_f1 = f1_score(y_test, iso_forest_preds)\n",
        "\n",
        "svm_precision = precision_score(y_test, svm_preds)\n",
        "svm_recall = recall_score(y_test, svm_preds)\n",
        "svm_f1 = f1_score(y_test, svm_preds)\n",
        "\n",
        "lof_precision = precision_score(y_test, lof_preds)\n",
        "lof_recall = recall_score(y_test, lof_preds)\n",
        "lof_f1 = f1_score(y_test, lof_preds)\n",
        "\n",
        "# Combine all models using weighted voting with lower threshold to improve recall\n",
        "final_preds = ((0.5 * lstm_preds) + (0.3 * iso_forest_preds) + (0.2 * svm_preds)) > 0.3\n",
        "final_precision = precision_score(y_test, final_preds)\n",
        "final_recall = recall_score(y_test, final_preds)\n",
        "final_f1 = f1_score(y_test, final_preds)\n",
        "\n",
        "# Print model comparison results\n",
        "print(\"Model Comparison Results:\")\n",
        "print(f\"LSTM Autoencoder - Precision: {lstm_precision:.4f}, Recall: {lstm_recall:.4f}, F1 Score: {lstm_f1:.4f}\")\n",
        "print(f\"Isolation Forest - Precision: {iso_precision:.4f}, Recall: {iso_recall:.4f}, F1 Score: {iso_f1:.4f}\")\n",
        "print(f\"One-Class SVM - Precision: {svm_precision:.4f}, Recall: {svm_recall:.4f}, F1 Score: {svm_f1:.4f}\")\n",
        "print(f\"Local Outlier Factor - Precision: {lof_precision:.4f}, Recall: {lof_recall:.4f}, F1 Score: {lof_f1:.4f}\")\n",
        "print(f\"Hybrid Model (Weighted, Lower Threshold) - Precision: {final_precision:.4f}, Recall: {final_recall:.4f}, F1 Score: {final_f1:.4f}\")\n",
        "\n",
        "print(\"Final optimized models trained and saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "FBV8q-QVMK1a",
        "outputId": "bd4050f9-55e9-41d7-ea23-a115b6203c57"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '2025-02-08 05:37:01.546913'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-aef013307a6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m# Clean dataset: Adjust DBSCAN to avoid removing true anomalies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0mdbscan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBSCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdbscan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cluster\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Keep only well-defined clusters (remove excessive filtering)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/cluster/_dbscan.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mfitted\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mof\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \"\"\"\n\u001b[0;32m--> 391\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1053\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    837\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 839\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    840\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     ) -> np.ndarray:\n\u001b[1;32m   2152\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2153\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2154\u001b[0m         if (\n\u001b[1;32m   2155\u001b[0m             \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2025-02-08 05:37:01.546913'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train LSTM Autoencoder & Isolation Forest for anomaly detection:"
      ],
      "metadata": {
        "id": "6a6b0YPxmYvl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import joblib\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the synthetic dataset\n",
        "df = pd.read_csv(\"sample_data/synthetic_azure_ad_logs.csv\")\n",
        "\n",
        "# Extract time-based features\n",
        "df[\"hour\"] = pd.to_datetime(df[\"timestamp\"]).dt.hour\n",
        "df[\"day_of_week\"] = pd.to_datetime(df[\"timestamp\"]).dt.dayofweek\n",
        "\n",
        "# Encode categorical features\n",
        "label_encoders = {}\n",
        "for col in [\"user_principal_name\", \"device_type\", \"location\", \"authentication_type\"]:\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "    label_encoders[col] = le\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = MinMaxScaler()\n",
        "df[[\"user_principal_name\", \"device_type\", \"location\", \"authentication_type\", \"hour\", \"day_of_week\"]] = scaler.fit_transform(\n",
        "    df[[\"user_principal_name\", \"device_type\", \"location\", \"authentication_type\", \"hour\", \"day_of_week\"]])\n",
        "\n",
        "\n",
        "# Ensure 'label' column exists before DBSCAN\n",
        "if \"label\" not in df.columns:\n",
        "    df[\"label\"] = df[\"anomaly_type\"].apply(lambda x: 1 if x != \"None\" else 0)\n",
        "\n",
        "# Convert categorical values to numeric\n",
        "for col in [\"user_principal_name\", \"device_type\", \"location\", \"authentication_type\"]:\n",
        "    if df[col].dtype == \"object\":\n",
        "        df[col] = LabelEncoder().fit_transform(df[col])\n",
        "\n",
        "# Drop non-numeric columns before DBSCAN\n",
        "dbscan_features = df.drop(columns=[\"timestamp\", \"label\", \"anomaly_type\"], errors=\"ignore\")\n",
        "\n",
        "# Debugging step: Print available columns before DBSCAN\n",
        "print(\"Available columns before DBSCAN:\", dbscan_features.columns)\n",
        "\n",
        "# Apply DBSCAN only on numeric features\n",
        "dbscan = DBSCAN(eps=0.5, min_samples=5).fit(dbscan_features)\n",
        "df[\"cluster\"] = dbscan.labels_\n",
        "\n",
        "# Remove noisy data (DBSCAN assigns noise as -1)\n",
        "df = df[df[\"cluster\"] != -1]\n",
        "\n",
        "\n",
        "# Define labels for anomaly detection (1 = anomaly, 0 = normal)\n",
        "df[\"label\"] = df[\"anomaly_type\"].apply(lambda x: 1 if x != \"None\" else 0)\n",
        "\n",
        "# Split into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df.drop(columns=[\"anomaly_type\", \"timestamp\", \"label\", \"cluster\"]), df[\"label\"], test_size=0.2, random_state=42)\n",
        "\n",
        "# Define LSTM Autoencoder Model\n",
        "class LSTMAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True, dropout=0.3)\n",
        "        self.decoder = nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.encoder(x)\n",
        "        decoded, _ = self.decoder(hidden.repeat(x.shape[1], 1, 1))\n",
        "        return decoded.squeeze()  # Ensure correct output shape\n",
        "\n",
        "# Train LSTM Autoencoder\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_dim = X_train.shape[1]\n",
        "model = LSTMAutoencoder(input_dim).to(device)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
        "\n",
        "# Training loop with final shape fix\n",
        "for epoch in range(10):\n",
        "    optimizer.zero_grad()\n",
        "    output = model(X_train_tensor.unsqueeze(1)).squeeze()  # Ensure input shape is correct\n",
        "    loss = criterion(output, X_train_tensor)  # Ensure target and input shapes match\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item()}\")\n",
        "\n",
        "# Save the trained LSTM Autoencoder\n",
        "torch.save(model.state_dict(), \"lstm_autoencoder.pth\")\n",
        "\n",
        "# Train Isolation Forest with increased contamination to improve recall\n",
        "iso_forest = IsolationForest(contamination=0.40, random_state=42)\n",
        "iso_forest.fit(X_train)\n",
        "iso_forest_preds = iso_forest.predict(X_test)\n",
        "iso_forest_preds = np.where(iso_forest_preds == -1, 1, 0)\n",
        "\n",
        "# Train One-Class SVM with improved `nu` parameter\n",
        "svm_model = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.40)\n",
        "svm_model.fit(X_train)\n",
        "svm_preds = svm_model.predict(X_test)\n",
        "svm_preds = np.where(svm_preds == -1, 1, 0)\n",
        "\n",
        "# Train Local Outlier Factor (LOF) for additional anomaly detection\n",
        "lof_model = LocalOutlierFactor(n_neighbors=10, contamination=0.40)\n",
        "lof_preds = lof_model.fit_predict(X_test)\n",
        "lof_preds = np.where(lof_preds == -1, 1, 0)\n",
        "\n",
        "# Save the trained Isolation Forest, One-Class SVM, and LOF models\n",
        "joblib.dump(iso_forest, \"isolation_forest.pkl\")\n",
        "joblib.dump(svm_model, \"one_class_svm.pkl\")\n",
        "joblib.dump(lof_model, \"local_outlier_factor.pkl\")\n",
        "\n",
        "# Compute evaluation metrics for LSTM Autoencoder\n",
        "with torch.no_grad():\n",
        "    reconstructed = model(X_test_tensor.unsqueeze(1)).squeeze()\n",
        "    reconstruction_errors = torch.mean((X_test_tensor - reconstructed) ** 2, dim=1).cpu().numpy()\n",
        "\n",
        "# Ensure reconstruction_errors is 1D (length = num_samples)\n",
        "reconstruction_errors = reconstruction_errors.flatten()\n",
        "\n",
        "# Define a dynamic threshold for anomaly detection using LSTM Autoencoder (adaptive percentile)\n",
        "lstm_dynamic_thresholds = np.percentile(reconstruction_errors, np.linspace(50, 95, num=5))\n",
        "best_threshold = min(lstm_dynamic_thresholds, key=lambda x: abs(x - np.median(reconstruction_errors)))\n",
        "lstm_preds = (reconstruction_errors > best_threshold).astype(int)\n",
        "\n",
        "# Compute evaluation metrics\n",
        "lstm_precision = precision_score(y_test, lstm_preds)\n",
        "lstm_recall = recall_score(y_test, lstm_preds)\n",
        "lstm_f1 = f1_score(y_test, lstm_preds)\n",
        "\n",
        "iso_precision = precision_score(y_test, iso_forest_preds)\n",
        "iso_recall = recall_score(y_test, iso_forest_preds)\n",
        "iso_f1 = f1_score(y_test, iso_forest_preds)\n",
        "\n",
        "svm_precision = precision_score(y_test, svm_preds)\n",
        "svm_recall = recall_score(y_test, svm_preds)\n",
        "svm_f1 = f1_score(y_test, svm_preds)\n",
        "\n",
        "lof_precision = precision_score(y_test, lof_preds)\n",
        "lof_recall = recall_score(y_test, lof_preds)\n",
        "lof_f1 = f1_score(y_test, lof_preds)\n",
        "\n",
        "# Combine all models using weighted voting\n",
        "final_preds = ((0.5 * lstm_preds) + (0.3 * iso_forest_preds) + (0.2 * svm_preds)) > 0.5\n",
        "final_precision = precision_score(y_test, final_preds)\n",
        "final_recall = recall_score(y_test, final_preds)\n",
        "final_f1 = f1_score(y_test, final_preds)\n",
        "\n",
        "# Print model comparison results\n",
        "print(\"Model Comparison Results:\")\n",
        "print(f\"LSTM Autoencoder - Precision: {lstm_precision:.4f}, Recall: {lstm_recall:.4f}, F1 Score: {lstm_f1:.4f}\")\n",
        "print(f\"Isolation Forest - Precision: {iso_precision:.4f}, Recall: {iso_recall:.4f}, F1 Score: {iso_f1:.4f}\")\n",
        "print(f\"One-Class SVM - Precision: {svm_precision:.4f}, Recall: {svm_recall:.4f}, F1 Score: {svm_f1:.4f}\")\n",
        "print(f\"Local Outlier Factor - Precision: {lof_precision:.4f}, Recall: {lof_recall:.4f}, F1 Score: {lof_f1:.4f}\")\n",
        "print(f\"Hybrid Model (Weighted) - Precision: {final_precision:.4f}, Recall: {final_recall:.4f}, F1 Score: {final_f1:.4f}\")\n",
        "\n",
        "print(\"Final optimized models trained and saved successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0npptSm02_e",
        "outputId": "f283e427-e15b-41e4-8836-c0e3432afbfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available columns before DBSCAN: Index(['user_principal_name', 'device_type', 'location', 'authentication_type',\n",
            "       'hour', 'day_of_week'],\n",
            "      dtype='object')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.3550710380077362\n",
            "Epoch [2/10], Loss: 0.34819334745407104\n",
            "Epoch [3/10], Loss: 0.3412421643733978\n",
            "Epoch [4/10], Loss: 0.3342207968235016\n",
            "Epoch [5/10], Loss: 0.3271317481994629\n",
            "Epoch [6/10], Loss: 0.3199799656867981\n",
            "Epoch [7/10], Loss: 0.3127730190753937\n",
            "Epoch [8/10], Loss: 0.30552050471305847\n",
            "Epoch [9/10], Loss: 0.29823482036590576\n",
            "Epoch [10/10], Loss: 0.29093116521835327\n",
            "Model Comparison Results:\n",
            "LSTM Autoencoder - Precision: 1.0000, Recall: 0.5000, F1 Score: 0.6667\n",
            "Isolation Forest - Precision: 1.0000, Recall: 0.4074, F1 Score: 0.5789\n",
            "One-Class SVM - Precision: 1.0000, Recall: 0.4066, F1 Score: 0.5781\n",
            "Local Outlier Factor - Precision: 1.0000, Recall: 0.4000, F1 Score: 0.5714\n",
            "Hybrid Model (Weighted) - Precision: 1.0000, Recall: 0.2777, F1 Score: 0.4347\n",
            "Final optimized models trained and saved successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAST API"
      ],
      "metadata": {
        "id": "QdhR7uDitQON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, HTTPException\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import joblib\n",
        "from pydantic import BaseModel\n",
        "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
        "\n",
        "# Initialize FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Load trained models\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class LSTMAutoencoder(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(LSTMAutoencoder, self).__init__()\n",
        "        self.encoder = torch.nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
        "        self.decoder = torch.nn.LSTM(hidden_dim, input_dim, batch_first=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.encoder(x)\n",
        "        decoded, _ = self.decoder(hidden.repeat(x.shape[1], 1, 1))\n",
        "        return decoded\n",
        "\n",
        "# Load LSTM Autoencoder\n",
        "input_dim = 4  # Assuming 4 input features: user, device, location, authentication type\n",
        "model = LSTMAutoencoder(input_dim).to(device)\n",
        "model.load_state_dict(torch.load(\"lstm_autoencoder.pth\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "# Load Isolation Forest model\n",
        "iso_forest = joblib.load(\"isolation_forest.pkl\")\n",
        "\n",
        "# Define the input model for FastAPI request\n",
        "class LoginEvent(BaseModel):\n",
        "    user_principal_name: str\n",
        "    device_type: str\n",
        "    location: str\n",
        "    authentication_type: str\n",
        "\n",
        "# Predefined encoders for categorical variables (must match training data)\n",
        "label_encoders = {\n",
        "    \"user_principal_name\": LabelEncoder(),\n",
        "    \"device_type\": LabelEncoder(),\n",
        "    \"location\": LabelEncoder(),\n",
        "    \"authentication_type\": LabelEncoder(),\n",
        "}\n",
        "\n",
        "# Placeholder categories for encoding (should be updated based on trained model categories)\n",
        "categories = {\n",
        "    \"user_principal_name\": [f\"user_{i}@company.com\" for i in range(500)],\n",
        "    \"device_type\": [\"Workstation\", \"Mobile\", \"VPN\"],\n",
        "    \"location\": [\"New York\", \"San Francisco\", \"London\", \"Tokyo\", \"Berlin\"],\n",
        "    \"authentication_type\": [\"OAuth2\", \"SAML\", \"MFA\", \"NTLM\"]\n",
        "}\n",
        "\n",
        "# Fit encoders with predefined categories\n",
        "for col, values in categories.items():\n",
        "    label_encoders[col].fit(values)\n",
        "\n",
        "# MinMax Scaler (same scaling as training data)\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(np.array([[0, 0, 0, 0], [len(categories[\"user_principal_name\"]) - 1,\n",
        "                                    len(categories[\"device_type\"]) - 1,\n",
        "                                    len(categories[\"location\"]) - 1,\n",
        "                                    len(categories[\"authentication_type\"]) - 1]]))\n",
        "\n",
        "@app.post(\"/detect_anomaly/\")\n",
        "async def detect_anomaly(event: LoginEvent):\n",
        "    try:\n",
        "        # Encode categorical values\n",
        "        encoded_data = np.array([\n",
        "            label_encoders[\"user_principal_name\"].transform([event.user_principal_name])[0],\n",
        "            label_encoders[\"device_type\"].transform([event.device_type])[0],\n",
        "            label_encoders[\"location\"].transform([event.location])[0],\n",
        "            label_encoders[\"authentication_type\"].transform([event.authentication_type])[0],\n",
        "        ]).reshape(1, -1)\n",
        "\n",
        "        # Scale input data\n",
        "        encoded_data = scaler.transform(encoded_data)\n",
        "\n",
        "        # Convert to tensor for LSTM Autoencoder\n",
        "        X_tensor = torch.tensor(encoded_data, dtype=torch.float32).to(device)\n",
        "\n",
        "        # Compute reconstruction error\n",
        "        with torch.no_grad():\n",
        "            reconstructed = model(X_tensor.unsqueeze(1))\n",
        "            reconstruction_error = torch.mean((X_tensor.unsqueeze(1) - reconstructed) ** 2, dim=[1, 2]).cpu().numpy()\n",
        "\n",
        "        # Predict anomaly using Isolation Forest\n",
        "        iso_pred = iso_forest.predict(encoded_data)\n",
        "        iso_pred = 1 if iso_pred[0] == -1 else 0\n",
        "\n",
        "        # Define anomaly detection threshold (adjustable)\n",
        "        threshold = 0.02\n",
        "        lstm_anomaly_flag = 1 if reconstruction_error[0] > threshold else 0\n",
        "\n",
        "        # Final anomaly decision (if either model flags an anomaly)\n",
        "        final_anomaly = 1 if lstm_anomaly_flag or iso_pred else 0\n",
        "\n",
        "        return {\n",
        "            \"user_principal_name\": event.user_principal_name,\n",
        "            \"device_type\": event.device_type,\n",
        "            \"location\": event.location,\n",
        "            \"authentication_type\": event.authentication_type,\n",
        "            \"LSTM_Anomaly_Score\": float(reconstruction_error[0]),\n",
        "            \"Isolation_Forest_Anomaly\": iso_pred,\n",
        "            \"LSTM_Anomaly_Flag\": lstm_anomaly_flag,\n",
        "            \"Final_Anomaly\": final_anomaly\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "# Run the API with: uvicorn fastapi_anomaly_detection:app --host 0.0.0.0 --port 8000\n"
      ],
      "metadata": {
        "id": "LviNYdbZn0Hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "siMXXmz4nz5C"
      }
    }
  ]
}